{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m intensity_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_lidar_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m    100\u001b[0m reflectance_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_lidar_reflectance\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflectance_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m intensity \u001b[38;5;241m=\u001b[39m inverse_transform(intensity)\n\u001b[1;32m     64\u001b[0m gt_output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gt_output_folder, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(intensity_path))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGT\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/utils.py:146\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    145\u001b[0m     _log_api_usage_once(save_image)\n\u001b[0;32m--> 146\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[1;32m    148\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/utils.py:72\u001b[0m, in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# single image\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# if single-channel, convert to 3-channel\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# single-channel images\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CycleGAN #Model trained on VoxelScape prediction on #Kitti\n",
    "#Prediction from the test folder saving predicted images# Working with entire folder\n",
    "#binary mask from intensity image\n",
    "from transform_utils import lidar_transform,intensity_real_transform,incidence_transform, reflectance_transform\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from generator import Generator\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "\n",
    "\n",
    "def process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device):\n",
    "    # Load the input images\n",
    "    lidar = Image.open(lidar_path).convert(\"L\")\n",
    "    incidence = Image.open(incidence_path).convert(\"L\")\n",
    "    intensity = Image.open(intensity_path).convert(\"L\")\n",
    "    reflectance = Image.open(reflectance_path).convert(\"L\")\n",
    "\n",
    "    # Apply your transformations here...\n",
    "    # Assuming lidar_transform, incidence_transform, intensity_real_transform, and reflectance_transform are defined elsewhere\n",
    "    lidar = lidar_transform(lidar)\n",
    "    incidence = incidence_transform(incidence)\n",
    "    intensity = intensity_real_transform(intensity)\n",
    "    reflectance = reflectance_transform(reflectance)\n",
    "\n",
    "    def inverse_transform(tensor):\n",
    "        mean = 0.4257\n",
    "        std = 0.2276\n",
    "        inv_tensor = tensor * std + mean\n",
    "        return inv_tensor\n",
    "    \n",
    "    # Create a binary mask from the intensity image\n",
    "    binary_mask = (intensity > 0).float()  # Create binary mask from intensity tensor\n",
    "    binary_mask = binary_mask.to(device)\n",
    "\n",
    "    # Your model input construction here\n",
    "    input_data = torch.cat((lidar,incidence, reflectance), dim=0) # Example input\n",
    "    input_data = input_data.unsqueeze(0) \n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    output = inverse_transform(output)\n",
    "\n",
    "    # Define the default value for zero intensity areas\n",
    "    default_value = 0.2\n",
    "    # Prepare a tensor with the default value\n",
    "    default_tensor = torch.full_like(output, default_value)\n",
    "    # Use the binary mask to select between the model's output and the default value\n",
    "    output_masked = output * binary_mask + default_tensor * (1 - binary_mask)\n",
    "\n",
    "    # Apply binary mask to the output\n",
    "    #output_masked = output * binary_mask\n",
    "\n",
    "    # Save the output tensor as an image\n",
    "    output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(lidar_path))[0] + 'Pred' + '.png')\n",
    "    save_image(output_masked, output_file)\n",
    "\n",
    "    # Process and save the intensity image as ground truth\n",
    "    intensity = inverse_transform(intensity)\n",
    "    gt_output_file = os.path.join(gt_output_folder, os.path.splitext(os.path.basename(intensity_path))[0] + 'GT' + '.png')\n",
    "    save_image(intensity, gt_output_file)\n",
    "\n",
    "\n",
    "# specify your input and output directories\n",
    "Trial = \"T7\" #Change accordingly\n",
    "in_channels = 3 #Change accordingly\n",
    "\n",
    "input_folder = \"/DATA2/Vivek/Data/Trial_3/Test/\" #Change accordingly\n",
    "output_folder = f\"/DATA2/Vivek/Code/Implementation/CycleGAN/Predicted_Image/Kitti_{Trial}c\" #Change accordingly\n",
    "gt_output_folder = output_folder\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(output_folder):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Generator(img_channels=in_channels, out_channels=1)  # Initialize your model architecture\n",
    "checkpoint = torch.load(f'/DATA2/Vivek/Code/Implementation/CycleGAN/Output_1.0/{Trial}/Model/gen_r.pth.tar_{Trial}')\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#Iterate over the subdirectories inside the input folder\n",
    "subfolders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "reference_subfolder = \"test_lidar_depth\"\n",
    "reference_files = os.listdir(os.path.join(input_folder, reference_subfolder))\n",
    "I\n",
    "for filename in reference_files:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        lidar_path = os.path.join(input_folder, \"test_lidar_depth\", filename)\n",
    "        incidence_path = os.path.join(input_folder, \"test_incidence_mask\", filename)\n",
    "        intensity_path = os.path.join(input_folder, \"test_lidar_intensity\", filename)\n",
    "        reflectance_path = os.path.join(input_folder, \"test_lidar_reflectance\", filename)\n",
    "\n",
    "        \n",
    "        process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/DATA2/Vivek/Data/Trial_3/Train/train_lidar_reflectance/03_000713.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m reflectance_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_lidar_reflectance\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m    112\u001b[0m binary_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_binary_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m--> 114\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflectance_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(lidar_path, incidence_path, intensity_path, reflectance_path, binary_path, output_folder, gt_output_folder, model, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m incidence \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(incidence_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m intensity \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(intensity_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m reflectance \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreflectance_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m binary \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(binary_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply your transformations here...\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:3243\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3240\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3243\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3244\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/DATA2/Vivek/Data/Trial_3/Train/train_lidar_reflectance/03_000713.jpg'"
     ]
    }
   ],
   "source": [
    "#CycleGAN #Model trained on VoxelScape prediction on #Kitti\n",
    "#Prediction from the test folder saving predicted images# Working with entire folder\n",
    "#With Binary Mask\n",
    "from transform_utils import lidar_transform,intensity_real_transform,incidence_transform, reflectance_transform, binary_transform\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from generator import Generator\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def process_image(lidar_path, incidence_path, intensity_path, reflectance_path, binary_path, output_folder, gt_output_folder, model, device):\n",
    "    # Load the input images\n",
    "    lidar = Image.open(lidar_path).convert(\"L\")\n",
    "    incidence = Image.open(incidence_path).convert(\"L\")\n",
    "    intensity = Image.open(intensity_path).convert(\"L\")\n",
    "    reflectance = Image.open(reflectance_path).convert(\"L\")\n",
    "    binary = Image.open(binary_path).convert(\"L\")\n",
    "\n",
    "    # Apply your transformations here...\n",
    "\n",
    "    lidar = lidar_transform(lidar)\n",
    "    incidence = incidence_transform(incidence)\n",
    "    intensity = intensity_real_transform(intensity) \n",
    "    reflectance = reflectance_transform(reflectance)\n",
    "    binary = binary_transform(binary)\n",
    "\n",
    "    def inverse_transform(tensor):\n",
    "        mean = 0.4257\n",
    "        std = 0.2276\n",
    "        inv_tensor = tensor * std + mean\n",
    "        return inv_tensor\n",
    "    \n",
    "\n",
    "    # Kitti\n",
    "    #input_data = torch.cat((binary, label,lidar, color, rgb), dim=0)\n",
    "    #input_data = torch.cat((binary, label,incidence,lidar, color, rgb), dim=0)\n",
    "\n",
    "    #Voxelscape\n",
    "    #input_data = torch.cat((lidar,incidence), dim=0) #T4 #in_channels=2\n",
    "    #input_data = torch.cat((lidar,reflectance), dim=0) #T8 #in_channels=2\n",
    "    #input_data = torch.cat((lidar,incidence, reflectance), dim=0) #T7 #in_channels=3\n",
    "    input_data = torch.cat((lidar,reflectance), dim=0) #T8 #in_channels=2\n",
    "    #input_data = lidar #T6\n",
    "\n",
    "\n",
    "    input_data = input_data.unsqueeze(0) \n",
    "\n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    output = inverse_transform(output)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Convert the binary mask to a boolean array and apply it to the output tensor\n",
    "    binary_mask = (binary > 0).float()  # Create binary mask from intensity tensor\n",
    "    binary_mask = binary_mask.to(device)\n",
    "\n",
    "\n",
    "    # Apply binary mask to the output\n",
    "    output_masked = output * binary_mask\n",
    "\n",
    "    # Save the output tensor as an image\n",
    "    output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(lidar_path))[0] + 'Pred' + '.png')\n",
    "    save_image(output_masked, output_file)\n",
    "\n",
    "    # Process and save the intensity image as ground truth\n",
    "    intensity = inverse_transform(intensity)\n",
    "    gt_output_file = os.path.join(gt_output_folder, os.path.splitext(os.path.basename(intensity_path))[0] + 'GT' + '.png')\n",
    "    save_image(intensity, gt_output_file)\n",
    "    \n",
    "# specify your input and output directories\n",
    "Trial = \"T8\" #Change accordingly\n",
    "in_channels = 2 #Change accordingly\n",
    "\n",
    "input_folder = \"/DATA2/Vivek/Data/Trial_3/Train/\" #Change accordingly\n",
    "output_folder = f\"/DATA2/Vivek/Code/Implementation/CycleGAN/Predicted_Image/Kitti_{Trial}\" #Change accordingly\n",
    "gt_output_folder = output_folder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(output_folder):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Generator(img_channels=in_channels, out_channels=1)  # Initialize your model architecture\n",
    "checkpoint = torch.load(f'/DATA2/Vivek/Code/Implementation/CycleGAN/Output_1.0/{Trial}/Model/gen_r.pth.tar_{Trial}')\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#Iterate over the subdirectories inside the input folder\n",
    "subfolders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "reference_subfolder = \"train_lidar_depth\"\n",
    "reference_files = os.listdir(os.path.join(input_folder, reference_subfolder))\n",
    "\n",
    "for filename in reference_files:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        lidar_path = os.path.join(input_folder, \"train_lidar_depth\", filename)\n",
    "        incidence_path = os.path.join(input_folder, \"train_incidence_mask\", filename)\n",
    "        intensity_path = os.path.join(input_folder, \"train_lidar_intensity\", filename)\n",
    "        reflectance_path = os.path.join(input_folder, \"train_lidar_reflectance\", filename)\n",
    "        binary_path = os.path.join(input_folder, \"train_binary_mask\", filename)\n",
    "        \n",
    "        process_image(lidar_path, incidence_path, intensity_path, reflectance_path, binary_path, output_folder, gt_output_folder, model, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from generator import Generator  # Assuming this is where your Generator class is defined\n",
    "from transform_utils import lidar_transform, intensity_sim_transform, incidence_transform, label_transform\n",
    "\n",
    "binary_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(folder, in_channel):\n",
    "    model = Generator(img_channels=in_channel, out_channels=1)\n",
    "    checkpoint_path = f'/DATA2/Vivek/Code/Implementation/CycleGAN/Output_1.0/{folder}/Model/gen_r.pth.tar_{folder}'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def compute_masked_error(model, filename, input_channel_config):\n",
    "    paths = {img_type: os.path.join(base_path, subfolder, filename) for img_type, subfolder in zip(subfolders, subfolders)}\n",
    "    lidar = lidar_transform(Image.open(paths['test_lidar_depth']).convert(\"L\"))\n",
    "    intensity = intensity_sim_transform(Image.open(paths['test_lidar_intensity']).convert(\"L\"))\n",
    "    incidence = incidence_transform(Image.open(paths['test_incidence_mask']).convert(\"L\"))\n",
    "    binary = binary_transform(Image.open(paths['test_binary_mask']).convert(\"L\"))\n",
    "    label = label_transform(Image.open(paths['test_lidar_label']).convert(\"L\"))\n",
    "    \n",
    "    mask_tensor = (binary > 0.5).float()\n",
    "    intensity_target = intensity.unsqueeze(0).to(device)\n",
    "\n",
    "    def inverse_transform(tensor):\n",
    "        mean = 0.4257\n",
    "        std = 0.2276\n",
    "        return tensor * std + mean\n",
    "\n",
    "    if input_channel_config == \"T4\":\n",
    "        input_data = torch.cat((lidar, incidence), dim=0)\n",
    "    elif input_channel_config == \"T3\":\n",
    "        input_data = torch.cat((binary, incidence, lidar), dim=0)\n",
    "    elif input_channel_config == \"T2\":\n",
    "        input_data = torch.cat((lidar, incidence), dim=0)\n",
    "    else:  # \"T1\"\n",
    "        input_data = torch.cat((binary, label, incidence, lidar), dim=0)\n",
    "\n",
    "    input_data = input_data.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    intensity_target = inverse_transform(intensity_target)\n",
    "    output = inverse_transform(output)\n",
    "    error = abs(intensity_target - output)\n",
    "    error_np = error.squeeze().cpu().detach().numpy()\n",
    "    mask_np = mask_tensor.squeeze().cpu().detach().numpy()\n",
    "    mask_bool = mask_np > 0\n",
    "    masked_error_np = np.where(mask_bool, error_np, np.nan)\n",
    "    return masked_error_np\n",
    "\n",
    "base_path = \"/DATA2/Vivek/Data/Trial_3/Test\"\n",
    "subfolders = ['test_lidar_depth', 'test_incidence_mask', 'test_binary_mask', 'test_lidar_intensity', 'test_lidar_label']\n",
    "\n",
    "folder1 = \"T8\"\n",
    "in_channel1 = 2\n",
    "model1 = load_model(folder1, in_channel1)\n",
    "\n",
    "errors_model1 = []\n",
    "\n",
    "for filename in os.listdir(os.path.join(base_path, subfolders[0])):\n",
    "    if filename.endswith('.jpg'):\n",
    "        masked_error1 = compute_masked_error(model1, filename, folder1)\n",
    "        errors_model1.append(masked_error1)\n",
    "\n",
    "combined_errors_model1 = np.concatenate(errors_model1)\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "plt.hist(combined_errors_model1.flatten(), bins=50, alpha=0.7, color='royalblue', edgecolor='black', linewidth=0.7, label=\"With Physics Loss\")\n",
    "plt.xlabel('Prediction Error', fontsize=24)\n",
    "plt.xticks(np.arange(0, 0.7, 0.1), fontsize=18)\n",
    "plt.ylabel(f\"Frequency (in $10^4$)\", fontsize=24, labelpad=5)\n",
    "plt.title('Histogram of Prediction Errors for Model 1', fontsize=24)\n",
    "plt.legend(loc='upper right', fontsize=18)\n",
    "# Uncomment the next line to save the figure\n",
    "#plt.savefig(f'/DATA2/Vivek/Histogram_Model1_Test_Set.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
