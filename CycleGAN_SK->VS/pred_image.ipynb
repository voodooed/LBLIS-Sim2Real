{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m intensity_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lidar_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m     88\u001b[0m reflectance_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lidar_reflectance\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflectance_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m intensity \u001b[38;5;241m=\u001b[39m inverse_transform(intensity)\n\u001b[1;32m     50\u001b[0m gt_output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gt_output_folder, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(intensity_path))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGT\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/utils.py:146\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    145\u001b[0m     _log_api_usage_once(save_image)\n\u001b[0;32m--> 146\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[1;32m    148\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/utils.py:72\u001b[0m, in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# single image\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# if single-channel, convert to 3-channel\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# single-channel images\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CycleGAN #Model trained on VoxelScape + SemanticKitti prediction on #Kitti\n",
    "#Prediction from the test folder saving predicted images# Working with entire folder\n",
    "\n",
    "from transform_utils import lidar_transform,intensity_real_transform,incidence_transform, reflectance_transform\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from generator import Generator\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "\n",
    "\n",
    "def process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device):\n",
    "    # Load the input images\n",
    "    lidar = Image.open(lidar_path).convert(\"L\")\n",
    "    incidence = Image.open(incidence_path).convert(\"L\")\n",
    "    intensity = Image.open(intensity_path).convert(\"L\")\n",
    "    reflectance = Image.open(reflectance_path).convert(\"L\")\n",
    "\n",
    "    # Apply your transformations here...\n",
    "    # Assuming lidar_transform, incidence_transform, intensity_real_transform, and reflectance_transform are defined elsewhere\n",
    "    lidar = lidar_transform(lidar)\n",
    "    incidence = incidence_transform(incidence)\n",
    "    intensity = intensity_real_transform(intensity)\n",
    "    reflectance = reflectance_transform(reflectance)\n",
    "\n",
    "    def inverse_transform(tensor):\n",
    "        mean = 0.4257\n",
    "        std = 0.2276\n",
    "        inv_tensor = tensor * std + mean\n",
    "        return inv_tensor\n",
    "    \n",
    "    # Your model input construction here\n",
    "    input_data = torch.cat((lidar,incidence, reflectance), dim=0) # Example input\n",
    "    input_data = input_data.unsqueeze(0) \n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    output = inverse_transform(output)\n",
    "\n",
    "    # Save the output tensor as an image\n",
    "    output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(lidar_path))[0] + 'Pred' + '.png')\n",
    "    save_image(output, output_file)\n",
    "\n",
    "    # Process and save the intensity image as ground truth\n",
    "    intensity = inverse_transform(intensity)\n",
    "    gt_output_file = os.path.join(gt_output_folder, os.path.splitext(os.path.basename(intensity_path))[0] + 'GT' + '.png')\n",
    "    save_image(intensity, gt_output_file)\n",
    "\n",
    "\n",
    "# specify your input and output directories\n",
    "Trial = \"T7\" #Change accordingly\n",
    "in_channels = 3 #Change accordingly\n",
    "\n",
    "#input_folder = \"/DATA2/Vivek/Data/Trial_3/Train/\" #Change accordingly\n",
    "input_folder = '/DATA2/Vivek/Vivekk/Data/VoxelScape/Data/Test'\n",
    "output_folder = f\"/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/Plots/Pred_Image/Output_2.0/Voxel_{Trial}\" #Change accordingly\n",
    "gt_output_folder = output_folder\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(output_folder):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Generator(img_channels=in_channels, out_channels=1)  # Initialize your model architecture\n",
    "checkpoint = torch.load(f'/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/Output_2.0/{Trial}/Model/gen_r.pth.tar_{Trial}')\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#Iterate over the subdirectories inside the input folder\n",
    "subfolders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "folder = 'test'\n",
    "reference_subfolder = f\"{folder}_lidar_depth\"\n",
    "reference_files = os.listdir(os.path.join(input_folder, reference_subfolder))\n",
    "\n",
    "for filename in reference_files:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        lidar_path = os.path.join(input_folder, f\"{folder}_lidar_depth\", filename)\n",
    "        incidence_path = os.path.join(input_folder, f\"{folder}_incidence_mask\", filename)\n",
    "        intensity_path = os.path.join(input_folder, f\"{folder}_lidar_intensity\", filename)\n",
    "        reflectance_path = os.path.join(input_folder, f\"{folder}_lidar_reflectance\", filename)\n",
    "\n",
    "        \n",
    "        process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m intensity_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lidar_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m     88\u001b[0m reflectance_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lidar_reflectance\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflectance_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m lidar \u001b[38;5;241m=\u001b[39m lidar_transform(lidar)\n\u001b[1;32m     24\u001b[0m incidence \u001b[38;5;241m=\u001b[39m incidence_transform(incidence)\n\u001b[0;32m---> 25\u001b[0m intensity \u001b[38;5;241m=\u001b[39m \u001b[43mintensity_real_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m reflectance \u001b[38;5;241m=\u001b[39m reflectance_transform(reflectance)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(tensor):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CycleGAN_1.0 #Model trained on VoxelScape prediction on VoxelScape\n",
    "#Prediction from the test folder saving predicted images# Working with entire folder\n",
    "\n",
    "from transform_utils import lidar_transform,intensity_real_transform,incidence_transform, reflectance_transform\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from generator import Generator\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "\n",
    "\n",
    "def process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device):\n",
    "    # Load the input images\n",
    "    lidar = Image.open(lidar_path).convert(\"L\")\n",
    "    incidence = Image.open(incidence_path).convert(\"L\")\n",
    "    intensity = Image.open(intensity_path).convert(\"L\")\n",
    "    reflectance = Image.open(reflectance_path).convert(\"L\")\n",
    "\n",
    "    # Apply your transformations here...\n",
    "    # Assuming lidar_transform, incidence_transform, intensity_real_transform, and reflectance_transform are defined elsewhere\n",
    "    lidar = lidar_transform(lidar)\n",
    "    incidence = incidence_transform(incidence)\n",
    "    intensity = intensity_real_transform(intensity)\n",
    "    reflectance = reflectance_transform(reflectance)\n",
    "\n",
    "    def inverse_transform(tensor):\n",
    "        mean = 0.4257\n",
    "        std = 0.2276\n",
    "        inv_tensor = tensor * std + mean\n",
    "        return inv_tensor\n",
    "    \n",
    "    # Your model input construction here\n",
    "    input_data = torch.cat((lidar,incidence, reflectance), dim=0) # Example input\n",
    "    input_data = input_data.unsqueeze(0) \n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    output = inverse_transform(output)\n",
    "\n",
    "    # Save the output tensor as an image\n",
    "    output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(lidar_path))[0] + 'Pred' + '.png')\n",
    "    save_image(output, output_file)\n",
    "\n",
    "    # Process and save the intensity image as ground truth\n",
    "    intensity = inverse_transform(intensity)\n",
    "    gt_output_file = os.path.join(gt_output_folder, os.path.splitext(os.path.basename(intensity_path))[0] + 'GT' + '.png')\n",
    "    save_image(intensity, gt_output_file)\n",
    "\n",
    "\n",
    "# specify your input and output directories\n",
    "Trial = \"T7\" #Change accordingly\n",
    "in_channels = 3 #Change accordingly\n",
    "\n",
    "#input_folder = \"/DATA2/Vivek/Data/Trial_3/Train/\" #Change accordingly\n",
    "input_folder = '/DATA2/Vivek/Vivekk/Data/VoxelScape/Data/Val'\n",
    "output_folder = f\"/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/Plots/Pred_Image/Output_2.0/Voxel__cyclegan_1.0_{Trial}\" #Change accordingly\n",
    "gt_output_folder = output_folder\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(output_folder):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Generator(img_channels=in_channels, out_channels=1)  # Initialize your model architecture\n",
    "checkpoint = torch.load(f'/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/Output_1.0/{Trial}/Model/gen_r.pth.tar_{Trial}')\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#Iterate over the subdirectories inside the input folder\n",
    "subfolders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "folder = 'val'\n",
    "reference_subfolder = f\"{folder}_lidar_depth\"\n",
    "reference_files = os.listdir(os.path.join(input_folder, reference_subfolder))\n",
    "\n",
    "for filename in reference_files:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        lidar_path = os.path.join(input_folder, f\"{folder}_lidar_depth\", filename)\n",
    "        incidence_path = os.path.join(input_folder, f\"{folder}_incidence_mask\", filename)\n",
    "        intensity_path = os.path.join(input_folder, f\"{folder}_lidar_intensity\", filename)\n",
    "        reflectance_path = os.path.join(input_folder, f\"{folder}_lidar_reflectance\", filename)\n",
    "\n",
    "        \n",
    "        process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m reflectance_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lidar_reflectance\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m    100\u001b[0m binary_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_binary_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflectance_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbinary_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(lidar_path, incidence_path, intensity_path, reflectance_path, binary_path, output_folder, gt_output_folder, model, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Save the output tensor as an image\u001b[39;00m\n\u001b[1;32m     52\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lidar_path))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPred\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Process and save the intensity image as ground truth\u001b[39;00m\n\u001b[1;32m     56\u001b[0m intensity \u001b[38;5;241m=\u001b[39m inverse_transform(intensity)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/utils.py:148\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m grid \u001b[38;5;241m=\u001b[39m make_grid(tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    149\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(ndarr)\n\u001b[1;32m    150\u001b[0m im\u001b[38;5;241m.\u001b[39msave(fp, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#UNET#Model trained on VoxelScape prediction on \n",
    "#Prediction from the test folder saving predicted images# Working with entire folder\n",
    "\n",
    "from transform_utils import lidar_transform,intensity_real_transform,incidence_transform, reflectance_transform\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from generator import Generator\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from model import UNET\n",
    "from torchvision import transforms\n",
    "\n",
    "binary_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def process_image(lidar_path, incidence_path, intensity_path, reflectance_path,binary_path , output_folder, gt_output_folder, model, device):\n",
    "    # Load the input images\n",
    "    lidar = Image.open(lidar_path).convert(\"L\")\n",
    "    incidence = Image.open(incidence_path).convert(\"L\")\n",
    "    intensity = Image.open(intensity_path).convert(\"L\")\n",
    "    reflectance = Image.open(reflectance_path).convert(\"L\")\n",
    "    binary = binary_transform(Image.open(binary_path ).convert(\"L\"))\n",
    "\n",
    "    # Apply your transformations here...\n",
    "    # Assuming lidar_transform, incidence_transform, intensity_real_transform, and reflectance_transform are defined elsewhere\n",
    "    lidar = lidar_transform(lidar)\n",
    "    incidence = incidence_transform(incidence)\n",
    "    intensity = intensity_real_transform(intensity)\n",
    "    reflectance = reflectance_transform(reflectance)\n",
    "\n",
    "    def inverse_transform(tensor):\n",
    "        mean = 0.4257\n",
    "        std = 0.2276\n",
    "        inv_tensor = tensor * std + mean\n",
    "        return inv_tensor\n",
    "    \n",
    "    # Your model input construction here\n",
    "    input_data = torch.cat((binary,lidar,incidence, reflectance), dim=0) # Example input\n",
    "    input_data = input_data.unsqueeze(0) \n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    output = inverse_transform(output)\n",
    "\n",
    "    # Save the output tensor as an image\n",
    "    output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(lidar_path))[0] + 'Pred' + '.png')\n",
    "    save_image(output, output_file)\n",
    "\n",
    "    # Process and save the intensity image as ground truth\n",
    "    intensity = inverse_transform(intensity)\n",
    "    gt_output_file = os.path.join(gt_output_folder, os.path.splitext(os.path.basename(intensity_path))[0] + 'GT' + '.png')\n",
    "    save_image(intensity, gt_output_file)\n",
    "\n",
    "\n",
    "# specify your input and output directories\n",
    "Trial = \"T7\" #Change accordingly\n",
    "in_channels = 4 #Change accordingly\n",
    "\n",
    "#input_folder = \"/DATA2/Vivek/Data/Trial_3/Train/\" #Change accordingly\n",
    "input_folder = '/DATA2/Vivek/Vivekk/Data/VoxelScape/Data/Val'\n",
    "output_folder = f\"/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/Plots/Pred_Image/Output_2.0/Voxel__UNET_1.0_{Trial}\" #Change accordingly\n",
    "gt_output_folder = output_folder\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(output_folder):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNET(in_channels=4, out_channels=1)\n",
    "Checkpoint_path = f'/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/For Comparison with SOTA/Output_SOTA/UNET/T1/T1_model.pth.tar' #UNET\n",
    "\n",
    "#model = Generator_IA_MR(in_channels=in_channel, out_channels=1) \n",
    "#Checkpoint_path = f'/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/For Comparison with SOTA/GAN/gen.pth.tar_T5_epoch_200' #GAN\n",
    "checkpoint = torch.load(Checkpoint_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#Iterate over the subdirectories inside the input folder\n",
    "subfolders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "folder = 'val'\n",
    "reference_subfolder = f\"{folder}_lidar_depth\"\n",
    "reference_files = os.listdir(os.path.join(input_folder, reference_subfolder))\n",
    "\n",
    "for filename in reference_files:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        lidar_path = os.path.join(input_folder, f\"{folder}_lidar_depth\", filename)\n",
    "        incidence_path = os.path.join(input_folder, f\"{folder}_incidence_mask\", filename)\n",
    "        intensity_path = os.path.join(input_folder, f\"{folder}_lidar_intensity\", filename)\n",
    "        reflectance_path = os.path.join(input_folder, f\"{folder}_lidar_reflectance\", filename)\n",
    "        binary_path = os.path.join(input_folder, f\"{folder}_binary_mask\", filename)\n",
    "\n",
    "        \n",
    "        process_image(lidar_path, incidence_path, intensity_path, reflectance_path,binary_path , output_folder, gt_output_folder, model, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m intensity_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lidar_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m     88\u001b[0m reflectance_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lidar_reflectance\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincidence_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflectance_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_output_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m reflectance \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(reflectance_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply your transformations here...\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Assuming lidar_transform, incidence_transform, intensity_real_transform, and reflectance_transform are defined elsewhere\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m lidar \u001b[38;5;241m=\u001b[39m \u001b[43mlidar_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m incidence \u001b[38;5;241m=\u001b[39m incidence_transform(incidence)\n\u001b[1;32m     25\u001b[0m intensity \u001b[38;5;241m=\u001b[39m intensity_real_transform(intensity)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Pix2Pix #Model trained on VoxelScape + SemanticKitti prediction on #Kitti\n",
    "#Prediction from the test folder saving predicted images# Working with entire folder\n",
    "\n",
    "from transform_utils import lidar_transform,intensity_real_transform,incidence_transform, reflectance_transform\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from generator_IA_MR import Generator\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "\n",
    "\n",
    "def process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device):\n",
    "    # Load the input images\n",
    "    lidar = Image.open(lidar_path).convert(\"L\")\n",
    "    incidence = Image.open(incidence_path).convert(\"L\")\n",
    "    intensity = Image.open(intensity_path).convert(\"L\")\n",
    "    reflectance = Image.open(reflectance_path).convert(\"L\")\n",
    "\n",
    "    # Apply your transformations here...\n",
    "    # Assuming lidar_transform, incidence_transform, intensity_real_transform, and reflectance_transform are defined elsewhere\n",
    "    lidar = lidar_transform(lidar)\n",
    "    incidence = incidence_transform(incidence)\n",
    "    intensity = intensity_real_transform(intensity)\n",
    "    reflectance = reflectance_transform(reflectance)\n",
    "\n",
    "    def inverse_transform(tensor):\n",
    "        mean = 0.4257\n",
    "        std = 0.2276\n",
    "        inv_tensor = tensor * std + mean\n",
    "        return inv_tensor\n",
    "    \n",
    "    # Your model input construction here\n",
    "    input_data = torch.cat((lidar,incidence, reflectance), dim=0) # Example input\n",
    "    input_data = input_data.unsqueeze(0) \n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    output = inverse_transform(output)\n",
    "\n",
    "    # Save the output tensor as an image\n",
    "    output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(lidar_path))[0] + 'Pred' + '.png')\n",
    "    save_image(output, output_file)\n",
    "\n",
    "    # Process and save the intensity image as ground truth\n",
    "    intensity = inverse_transform(intensity)\n",
    "    gt_output_file = os.path.join(gt_output_folder, os.path.splitext(os.path.basename(intensity_path))[0] + 'GT' + '.png')\n",
    "    save_image(intensity, gt_output_file)\n",
    "\n",
    "\n",
    "# specify your input and output directories\n",
    "Trial = \"T1_Pix2Pix\" #Change accordingly\n",
    "in_channels = 3 #Change accordingly\n",
    "\n",
    "#input_folder = \"/DATA2/Vivek/Data/Trial_3/Train/\" #Change accordingly\n",
    "input_folder = '/DATA2/Vivek/Vivekk/Data/VoxelScape/Data/Val'\n",
    "output_folder = f\"/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/Plots/Pred_Image/Output_2.0/Voxel_{Trial}\" #Change accordingly\n",
    "gt_output_folder = output_folder\n",
    "\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(output_folder):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Generator(in_channels=in_channels, out_channels=1)  # Initialize your model architecture\n",
    "checkpoint = torch.load(f'/DATA2/Vivek/Code/Implementation/Objective 3/CycleGAN/For Comparison with SOTA/Output_SOTA/GAN/T1/Model/gen.pth.tar_T1_epoch_200')\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#Iterate over the subdirectories inside the input folder\n",
    "subfolders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "folder = 'val'\n",
    "reference_subfolder = f\"{folder}_lidar_depth\"\n",
    "reference_files = os.listdir(os.path.join(input_folder, reference_subfolder))\n",
    "\n",
    "for filename in reference_files:\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        lidar_path = os.path.join(input_folder, f\"{folder}_lidar_depth\", filename)\n",
    "        incidence_path = os.path.join(input_folder, f\"{folder}_incidence_mask\", filename)\n",
    "        intensity_path = os.path.join(input_folder, f\"{folder}_lidar_intensity\", filename)\n",
    "        reflectance_path = os.path.join(input_folder, f\"{folder}_lidar_reflectance\", filename)\n",
    "\n",
    "        \n",
    "        process_image(lidar_path, incidence_path, intensity_path, reflectance_path, output_folder, gt_output_folder, model, device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
